# -*- coding: utf-8 -*-
"""02_neural_network_classification_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_JqwNu8fJype1n0CMkkOMA9UmZmKj2F9

# 02. Neural network classification with pytorch

Classification is a problem of predicting whether something is one thing or another)

## 1. Make classification data and get it ready
"""

import sklearn
#skikit learn

from sklearn.datasets import make_circles

# Make 1000 samples
n_samples = 1000

# Create circles
# X is matrix/features, y label
X, y = make_circles(n_samples,
                    noise=0.03,
                    random_state=42)

len(X), len(y)

print(f"First 5 samples of X:\n{X[:5]}")
print(f"First 5 samples of y:\n{y[:5]}")

# Make DataFrame of circle data
import pandas as pd
circles = pd.DataFrame({"X1": X[:,0],
                        "X2": X[:, 1],
                        "label": y})
circles.head(10)

circles.label.value_counts()

# Visualize
import matplotlib.pyplot as plt

plt.scatter(x=X[:,0],
            y=X[:,1],
            c=y,
            cmap=plt.cm.RdYlBu)

"""Note: the data we're working with is often referred to as a toy dataset, a dataset that is small enough to experiment on but still sizable enoguh to practice the fundamentals

### 1.1 Check input and output shapes
"""

X.shape, y.shape
# these are numpy arrays, have to convert to Tensors

# View the first example of features and labels
X_sample = X[0]
y_sample = y[0]

print(f"Value for one sample of X:{X_sample} and the same for y {y_sample}")
print(f"Shape for one sample of X:{X_sample.shape} and the same for y {y_sample.shape}")

"""### 1.2 Turn data into tensors and creat train/test splits

"""

# Turn data into tensors
import torch

type(X)
X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

X[:5], y[:5]

# Split data into training and test sets

# split data train/test

from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2, # 20% of data will be test and 80% will be train
                                                    random_state=42)
len(X_train), len(X_test), len(y_train), len(y_test)

n_samples

"""## 2. Building a model

Let's build a model to classify our blue and red dots.

To do so, we want to:
1. SEtup device agnostic code so our code will run on an accelerator (GPU) if there is one
2. Construct a model (by subsclasing `nn.Module`)
3. Define a loss function and optimizer
4. Create a training and test loop
"""

# Device agnostic code
import torch
from torch import nn
# Make device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device

X_train.shape

"""Now we've setup device agnostic code, let's create a model that:

1. Subclasses `nn.module` (almost all models in pytorch subclass `nn.Module`)
2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data
3. Define a `forward()` method that outlines the forward pass (or forward computation) of the model
4. Instantiate an instance of our model class and send it to the target device
"""

# 1. Construct a model that subclasses nn.module
class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()
    # Create 2 nn.Linear layers capable of handling the shapes of our data
    self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features, and upscales to 5 features
    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature (same shape as y )


  # 3. Define a forward method that outlines the forward pass
  def forward(self, x):
    return self.layer_2(self.layer_1(x)) # x -> layer_1 --> layer_2 --> output


  # 4. Instantiate an instance of our model class and send it to the target device

model_0 = CircleModelV1().to(device)
model_0



next(model_0.parameters()).device

# Let's replicate the model above using nn.Sequential()

# goes thru
model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)
model_0

# Make predictions with the model
with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))
print(f"Length of predictions: {len(untrained_preds)} Shape: {untrained_preds.shape}")
print(f"Length of test sampeles: {X_test} Shape: {X_test.shape}")
print(f"\nFirst 10 predictions: {untrained_preds[:10]}")
print(f"\nFirst 10 labels: \n{y_test[:10]}")

y_test[:10], X_test[:10]

"""### 2.1 Setup loss function and optimizer

Which loss function or optimizer should we use?

Again this is problem specific.

For example, with regression (picking a number) you might want MAE or MSE (Mean Absolute Error, or Mean Squared Error)

For classification, you might want binary cross entropy or categorical cross entropy (cross entropy)


As a reminder, the loss function measures how wrong your models predictions are.

And for optimizers, two of the most common and useful are  SGD and ADam, however Pytorch has many other options built in.

For the loss function, we'll use `torch.nn.BECWithLogitsLoss()`, for more on what binary cross entropy is (BCE) check out online.
"""

# Setup loss func

# loss_fn = nn.BCELoss() # BCELoss = requires intputs to have gone thru the sigmoid activation function prior to BCELOSS
# nn.Sequential(
#     nn.Sigmoid(),
#     nn.BCELoss()
# )

# This versin is more stable also because it does multiple steps at once.
loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmod activation function built-in


optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1
                            ) # gradient descent

# Calculate accuracy - out of 100 examples, what percentage does our model get right?
def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred) # this function just compares how many values are the same from 2 parameters
  acc = (correct/len(y_pred)) *100
  return acc

"""### 3. Train Model

To train our model, we're gonna need to build our training loop

1. forward pass
2. calculate loss
3. optimizer zero grad
4. loss backward (backpropogation)
5. optimizer step (gradient descent)

### 3.1 Going from raw logits --> prediction probabilities --> prediction labels

Our model outputs are going to be raw  **logits**.

We can convert these logits into prediction probabilities by passing them to some kind of activiation function e.g sigmoid or binary cross entropy and **softmax** for multiclass.

Then we can convert our model's prediction probabilities to **prediction labels** by either rounding them or taking the `argmax`. Round is for binary, argmax is for softmax or multiclass.
"""

# View the first 5 outputs of the forward pass on the test data
model_0.eval()
with torch.inference_mode():
  y_logits = model_0(X_test.to(device))[:5]
y_logits

"""This isnt part of the course, but basically the logits are the RAW output we get from the training and then we are trying to convert them to the 0s and 1s because this is what our Y test data is in so we have to compare them that way

For our prediction probability values, we need to perform a range-style rounding on them:
* `y_pred_probs` >= 0.5, y = 1 (clas 1)
* `y_pred_probs` < 0.5, y = 0 (class 0)
"""

# Use the sigmoid activation function on our model logits to turn them into prediction probabilities
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs
# now we can pass these to a torch.round func
# find the predicted labels

y_preds = torch.round(y_pred_probs)
# in full (logits --> pred probs --> pred labels)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

# Check for equality
print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))

# Get rid of extra dimension
y_preds.squeeze()

"""### 3.2 Building a training and testing loop"""

torch.manual_seed(42)
torch.cuda.manual_seed(42)

# St the number of epochs
epochs = 1000

# Put data to target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Build training and evaluation loop
for epoch in range(epochs):
  ### training
  model_0.train()

  #1. Forward pass
  y_logits = model_0(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits --> pred probs --> pred labels

  # 2. calculate the loss / accuracy, this loss fn expects logits as an input
  loss = loss_fn(y_logits,y_train)
  acc = accuracy_fn(y_true=y_train, y_pred = y_pred)


  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Loss backward (backprop)
  loss.backward()

  # 5. Optimizer step (gradient descent)
  optimizer.step()


  ### Testing
  model_0.eval()
  with torch.inference_mode():
    #1. Forward pass
    test_logits = model_0(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    # 2. Calculate the test loss
    test_loss = loss_fn(test_logits,
                        y_test)

    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)

    # Print out what's happening

  if epoch == 1:
      print(f"Epoch: {epoch}  Loss: {loss:.5f}, Acc: {acc}% | Test loss: {test_loss}, Test acc: {test_acc}")



import requests
from pathlib import Path

# Download helper functions from Learn PyTorch repo (if it's not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py")
  with open("helper_functions.py", "wb") as f:
    f.write(request.content)


from helper_functions import plot_predictions, plot_decision_boundary

# Plot decision boundary of the model

plt.figure(figsize=(12,6))
plt.subplot(1,2,1) #rows columns index
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train)
plt.subplot(1,2,2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test)

"""### 5. Improving a model (From a model perspective)

* Add more layers, give the model more chances to learn about patterns in the data
* add more hidden units - go from 5 (current) to 10 hidden units.
* Fit for longer
* Changing the activation functions
* Change the learning rate
* change the loss function


These options are all from a model's persspective because they deal directly with the model, rather than the data

Because these options are all values we can change, these are considered **hyperparameters**

Lets try and improving the model by
* adding more hidden units: 5 -> 10
* increase the number of layers: 2 -> 3
* increase # of epochs 100 -> 1000
"""

class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10,out_features=1)


  def forward(self, x):
    # z = self.layer_1(x)
    # z = self.layer_2(z)
    # z = self.layer_3(z)
    return self.layer_3(self.layer_2(self.layer_1(x))) # this way of writin operations leverages speed ups where possible behind the scenes


model_1 = CircleModelV1().to(device)
model_1

# Create a loss function
# This versin is more stable also because it does multiple steps at once.
loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmod activation function built-in



# Create an optimizer
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr=0.1
                            ) # gradient descent

# Write a training and eval loop for odel_1

torch.manual_seed(42)
torch.cuda.manual_seed(42)

# St the number of epochs
epochs = 1000

# Put data to target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(epochs):
  model_1.train()

   #1. Forward pass
  y_logits = model_1(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits)) # logits --> pred probabilities --> pred labels



  # 2. Calculate the loss / accuracy
  loss = loss_fn(y_logits,y_train)
  acc = accuracy_fn(y_true=y_train, y_pred = y_pred)

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. loss backwards  (backpropogation)
  loss.backward()

  # 5. Optimizre step (gradient descent)
  optimizer.step()


  ### Testing
  model_1.eval()
  with torch.inference_mode():
    # 1. Forward pass
    test_logits = model_1(X_test).squeeze() #rid of extra dimension
    test_pred = torch.round(torch.sigmoid(test_logits))
    # 2. Calculate the loss
    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)


# print out
  if epoch % 100 == 0:
    print(f"Epoch: {epoch}  Loss: {loss:.5f}, Acc:{acc:.2f}%  Test loss:{test_loss:.5f}, Test acc: {test_acc:.2f}")

# Plot decision boundary of the model

plt.figure(figsize=(12,6))
plt.subplot(1,2,1) #rows columns index
plt.title("Train")
plot_decision_boundary(model_1, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_1, X_test, y_test)